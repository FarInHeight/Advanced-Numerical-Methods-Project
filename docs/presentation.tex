\documentclass[10pt]{beamer}
\definecolor{title}{RGB}{1, 45, 112}
\definecolor{body}{RGB}{218, 233, 255}
\usetheme[progressbar=frametitle]{metropolis}
\usecolortheme{wolverine}
\usefonttheme{professionalfonts}
\setbeamertemplate{navigation symbols}{}
\setbeamercolor{block title}{bg=title,fg=white}
\setbeamercolor{block body}{bg=body!40,fg=black}
\setbeamertemplate{section in toc}[square]
\setbeamertemplate{subsection in toc}[square]

\usepackage[T1]{fontenc}
\usepackage{libertinus}
\usepackage{listings}

% settings for code listings (colors and appearence)
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{bm}
\usepackage{amsmath, amssymb}
\usepackage{ragged2e}
\justifying

\usepackage[backend=biber, style=verbose]{biblatex}
\addbibresource{references.bib}
\nocite{*}

\title{Singular Value Decomposition}
\subtitle{An application to Big Data}
\author{Davide Sferrazza}
\institute{Universit√† degli Studi di Palermo}
\date{\today}

\def\titlepage{%
  \usebeamertemplate{title page}%
}

\begin{document}
\maketitle

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Singular Value Decomposition}
\subsection{What is it?}

\begin{frame}
    \frametitle{Definition of SVD}
    \begin{theorem}
        Given a matrix $A \in \mathbb{R}^{m \times n}$, it can always be found a decomposition such that
        $$A = U \Sigma V^T$$
        where $U \in \mathbb{R}^{m \times m}$, $V \in \mathbb{R}^{n \times n}$ and $\Sigma \in \mathbb{R}^{m \times n}$.

        $U$ and $V$ are two orthogonal matrices and $\Sigma$ is a diagonal matrix, namely:
        $$ ( \Sigma )_{ij} = 
            \begin{cases}
                0, & i \ne j \\
                \sigma_i, & i = j
            \end{cases} 
        $$
        where $\sigma_1 \ge \sigma_2 \ge \ldots  \ge \sigma_p \ge 0$, $p = \min\{m, n\}$.
    \end{theorem}
\end{frame}

\begin{frame}
    \frametitle{Definition of SVD}
    The non-zero entries of $\Sigma$, denoted by $\sigma_i$, are called \textit{singular values}.

    They are arrenged in a nonincreasing order by convention.

    The column vectors $\bm{u_i}$ of $U$ are called \textit{left singular vectors} and those $\bm{v_i}$ of $V$ are called \textit{right singular vectors}. \bigskip
    
    Since in general $m \ne n$, we have: 
    $$ A = \sum_{i = 1}^{p} \bm{u_i} \sigma_i \bm{v_i}^T $$

\end{frame}

\begin{frame}{Definition of SVD}
    \begin{theorem}
        If for some $r$ such that $ 1 \le r < p $ we have
        $$ \sigma_1 \ge \ldots \ge \sigma_{r} > \sigma_{r + 1} = \ldots = \sigma_p = 0  $$
        then
        \begin{itemize}
            \item $rank(A) = r$
            \item $ A = \sum \limits_{i = 1}^{r} \bm{u_i} \sigma_i \bm{v_i}^T $
        \end{itemize}
    \end{theorem}

    This means that all other $p-r$ dimensions of matrix $A$ are linear combinations of the first $r$.
\end{frame}

\begin{frame}{Definition of SVD}
    \begin{block}{Lower rank approximation}
        Let $A \in \mathbb{R}^{m \times n}$ be a matrix whose rank is $rank(A) = r$. \\
        If for a fixed integer value $k < r$ we define
        \begin{equation}
            \label{eq:lower_rank}
            A_k = \sum \limits_{i = 1}^{k} \sigma_i \bm{u_i}  \bm{v_i}^T 
        \end{equation}
        and
        $$ \mathcal{B} = \left\{ B \in \mathbb{R}^{m \times n} : rank(B) = k \right\}$$
        then 
        $$ \min_{B \in \mathcal{B}} \left\lVert A - B \right\rVert _2 = \left\lVert A - A_k \right\rVert _2 = \sigma_{k + 1} $$
    \end{block}
        
    This result tell us that $A_k$ represents the best approximation (considering the \textit{spectral norm}) of rank $k$ of matrix $A$.
\end{frame}

\subsection{How can singular values be computed?}

\begin{frame}{Singular values computation}
    To compute the singular values, consider the transponse of $A$ given its decomposition:
    $$ A^T = (U \Sigma V^T)^T = V \Sigma^T U^T$$
    The symmetric matrix $A^T A$ is equal to:
    $$ A^T A = ( V \Sigma^T U^T )(U \Sigma V^T) = V \Sigma^T \Sigma V^T$$
    Furthermore, this equation can be written as:
    $$ A^T A V = V \Sigma^T \Sigma $$
    This means that the diagonal entries of the square matrix $ \Sigma^T \Sigma $, which are the square of the singular values, are the eigenvalues of matrix $A^T A$ and $ V $ is the matrix of eigenvectors.
\end{frame}

\begin{frame}{Singular values computation}
    Similarly, consider the product of $A A^T$. It is equal to:
    $$ A A^T = (U \Sigma V^T)( V \Sigma^T U^T ) = U \Sigma \Sigma^T U^T$$
    Which means that:
    $$ AA^T U = U \Sigma \Sigma^T $$
    Hence $U$ is the matrix of eigenvectors of $AA^T$. \bigskip

    Since $rank(A) = r$, only the first $r$ eigenvalues of $AA^T$ and $A^T A$ are non-zero.
\end{frame}

\section{Finding eigenvalues and eigenvectors}
\subsection{QR Method}

\begin{frame}{QR Method}
    A possible method to find eigenvalues and eigenvectors of a matrix is based on $QR$ decompositions and this theorem:
    \begin{theorem}
        Suppose $A \in \mathbb{R}^{n \times n}$ is a matrix having eigenvalues $\lambda_1, \lambda_2, \ldots \lambda_n$ satisfying 
        \begin{equation}
            \label{eq:eigenvalues_rel}
            \left\lvert \lambda_1 \right\rvert > \left\lvert \lambda_2 \right\rvert > \ldots > \left\lvert \lambda_n \right\rvert
        \end{equation}
        then the following sequence for $A_1 = A$ and $k = 1, 2, \ldots$
        \begin{equation}
            \label{eq:A_sequence}
            \begin{cases}
                A_k = Q_k R_k \\
                A_{k+1} = R_k Q_k 
            \end{cases} 
        \end{equation}
        converges to an upper triangular matrix where $(A_k)_{ii} = \lambda_i, \, i = 1, 2, \ldots, n$.
        In case (\ref{eq:eigenvalues_rel}) is not satisfied, this sequence converges to a triangular matrix with square blocks of order at most 2 along the diagonal. \newline
        If $A$ is symmetric, then the sequence converges to a diagonal matrix. 
    \end{theorem}
\end{frame}

\begin{frame}[fragile]{QR Method}
    So a basic implementation would be like this:
    \begin{lstlisting}[language=Matlab]
    while err < toll
        [Q, R] = qr(A);
        A = R * Q;

        err = max( max( tril(A, -1) ) );
    end \end{lstlisting}
    This method could be speed up using a technique called \textit{shifing}:
    \begin{lstlisting}[language=Matlab]
    n = length( A );
    while err < toll
        % A(n, n) is an usual choice, it could be any real number
        T = A(n, n) * eye(n);
        [Q, R] = qr( A - T );
        A = R * Q + T;

        err = max( max( tril(A, -1) ) );
    end \end{lstlisting}
\end{frame}

\begin{frame}{QR Method}
    In our case, this method must be applied to $A A^T$ and $A^T A$, so if (\ref{eq:A_sequence}) converges then we have a diagonal matrix. \newline
    If we consider the diagonalization of $B$, where $B = A A^T$ or $B = A^T A$:
    $$
    B = P \Lambda P^{-1} =  P \Lambda P^T
    $$
    As $B$ can be factorized using (\ref{eq:A_sequence}), the matrix containing the eigenvectors must be equal to:
    $$
    P = \prod_i Q_i = Q_1 Q_2 Q_3 \cdots
    $$ \bigskip

    Hence, for every iteration $B_k = Q_k R_k$ and $B_{k+1} = R_k Q_k$, requiring each step to have a computational cost equal to $O(\frac{2 n^3}{3})$.
\end{frame}

\subsection{Improving QR Method using Givens matrices}

\begin{frame}{Hessember Reduction}

\end{frame}

\begin{frame}[fragile, allowframebreaks]{Prova}
    \lstinputlisting[language=Matlab]{../givens.m}
\end{frame}

\section{References}
\begin{frame}[allowframebreaks, noframenumbering]{References}
    \printbibliography
\end{frame}

\end{document}